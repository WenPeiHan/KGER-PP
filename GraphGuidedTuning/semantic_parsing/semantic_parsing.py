#-*-coding -utf-8 -*-
import os
import json
from langchain.chat_models import ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain.graphs import Neo4jGraph
from sentence_transformers import SentenceTransformer
from typing import List
from tqdm import tqdm
import random
import argparse
from ChatGLM3 import ChatGLM3
from collections import defaultdict
from pathlib import Path
import time

random.seed(42)

# --data ../../DataSet/data.json --output output --model_path /root/autodl-tmp/PrML/chatglm3-6b --GLM --case_path case/case_50.json --graph_schema case/graph_schema.txt --case_num 5 --emb_model /root/autodl-tmp/PrML/all-MiniLM-L6-v2
# 20250219
"""
--data
case/output/data_wo_case_random_100.json
--output
output_change
--GPT
--temperature
0.3
--case_path
case/output/case_type_2.json
--graph_schema
case/graph_schema.txt
--case_num
5
--test_num
10
--generate_num
3
"""
def parse_args():
    parser = argparse.ArgumentParser(description='Parameters related to node embeddings.')
    parser.add_argument("--data", type=str, default="../DataSet/data.json", help="Data json file path", )
    parser.add_argument("--output", type=str, default="", help="Store path")
    parser.add_argument("--model_path", type=str, default=None, help="The Model Path")
    parser.add_argument("--emb_model",default=None,type=str)
    parser.add_argument("--GLM", action="store_true", default=False)
    parser.add_argument("--GPT",action="store_true", default=False)
    parser.add_argument("--temperature", default=0.3, type=float)
    parser.add_argument("-cp","--case_path",default="")
    parser.add_argument("--case_random",action="store_true")
    parser.add_argument("--graph_schema",default="")
    parser.add_argument("--case_num",default=5,type=int)
    parser.add_argument("--test_num",default=None,type=int)
    parser.add_argument("--generate_num",default=1,type=int,help="The number of generated queries. The results generated by the LLM for the same question are different. Let the LLM generate multiple semantic parsing results. ")
    parser.add_argument("--type_equal",action="store_true",help="Give the same weight to different types, that is, the amount of data in each type is the same. ")
    parser.add_argument("--etype_num", default=10,type=int)
    parser.add_argument("--wo_schema",action="store_true")
    parser.add_argument("--wo_ner",action="store_true")
    return parser.parse_args()


class LocalEmbeddings:
    def __init__(self, model):
        self.model = SentenceTransformer(model)
    # def embed_documents(self, texts: List[str]) -> List[List[float]]:
    #     return [self.model.encode(t).tolist() for t in texts]
    #
    # def embed_query(self, query: str) -> List[float]:
    #     return self.model.encode([query])
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # Ensure that the embeddings are lists of floats, not ndarray
        return [self.model.encode(t).tolist() for t in texts]

    def embed_query(self, query: str) -> List[float]:
        # Ensure that the embedding is a list of floats, not ndarray
        return self.model.encode([query])[0].tolist()

def read_json(dir):
    # 打开JSON文件并读取内容
    with open(dir, 'r', encoding='utf-8') as json_file:
        file_content = json_file.read()

    # 解析JSON字符串为字典
    data_dict = json.loads(file_content)
    return data_dict

def process_case(file_path):
    case_list = []
    cases_data = read_json(file_path)
    for case in cases_data:
        needed = dict()
        needed["question"] = case["paraphrased_questions"]
        needed["cypher"] = case["cypher"]
        case_list.append(needed)
    return case_list

def get_cypher(data_path, output, chain, args):
    data = read_json(data_path)
    if args.test_num:
        data = random.sample(data, args.test_num)
    # data = random.sample(data, 3)

    '''
    # Classify according to "type", and conduct semantic parsing after classification
    classified = defaultdict(list)
    specific_key = "questions_template"

    for item in data:
        # Check if the key exists in the dictionary
        if specific_key in item:
            classified[item[specific_key]].append(item)

    data_classified = dict(classified)
    data_concat = []

    for key, type_list in data_classified.items():
        try:
            for item in tqdm(type_list, total=len(type_list),desc="Getting Cypher With CASE, the question type is {}".format(key)):
                question = item["paraphrased_questions"]
                phs = item["placeholders_value"]
                cypher_list = []
                # for time in tqdm(range(args.generate_num), total=args.generate_num, desc="generate {} for cypher".format(args.generate_num)):
                for time in range(args.generate_num):
                    cypher = chain.invoke(
                        {
                            "question": question,
                            "schema": graph_schema,
                            "entities": phs
                         }
                    )
                    cypher_list.append(cypher)

                item["cypher_"] = cypher_list
                data_concat.append(item)

        except Exception as e:
            # Log the current key and error message
            print("Error occurred with key {}, \nerror information: \n{}".format(key, e))
            # continue  # Skip to the next key if an error occurs
            break'''

    if args.type_equal and args.etype_num:

        # Classify according to "type", and conduct semantic parsing after classification
        classified = defaultdict(list)
        specific_key = "questions_template"

        for item in data:
            # Check if the key exists in the dictionary
            if specific_key in item:
                classified[item[specific_key]].append(item)

        data_classified = dict(classified)
        data_concat = []

        for key, type_list in data_classified.items():
            if len(type_list) >= args.etype_num:
                type_list_random_select = random.sample(type_list, args.etype_num)
            else:
                # If the list has fewer than 100 elements, return a copy of the list
                type_list_random_select = type_list.copy()

            data_concat.extend(type_list_random_select)

        data = data_concat

    # Save data to a file
    if args.test_num == None:

        model_name = "GLM" if args.GLM else "GPT"
        case_random_select = "r" if args.case_random else "t"
        path = f"semantic_parse_{model_name}_{case_random_select}_case{{}}_gen{{}}.json"
        if args.wo_schema or args.wo_ner:
            path = f"semantic_parse_{model_name}_{case_random_select}_case{{}}_gen{{}}_{{}}.json"

        if args.wo_schema or args.wo_ner:
            if args.wo_schema:
                wo = "wo_schema"
            else:
                wo = "wo_ner"
            output_dir = os.path.join(output, path.format(args.case_num, args.generate_num,wo))
        elif args.GLM:
            # output_dir = os.path.join(output, "semantic_parse_GLM.json")
            output_dir = os.path.join(output, path.format(args.case_num, args.generate_num))
        elif args.GPT:
            output_dir = os.path.join(output, path.format(args.case_num, args.generate_num))
        else:
            raise ValueError("Not during the testing process, and the model type is not specified.")
    else:
        model_name = "GLM" if args.GLM else "GPT"
        output_dir = os.path.join(output, "semantic_parse_{}_test{}_case{}_gen{}.json".format(model_name, args.test_num, args.case_num, args.generate_num))

    previous_output_dir = output_dir

    if os.path.exists(output_dir):
        index = 1
        while True:
            new_output_dir = f"{os.path.splitext(output_dir)[0]}_{index}{os.path.splitext(output_dir)[1]}"
            if not os.path.exists(new_output_dir):
                output_dir = new_output_dir
                break
            index += 1

        previous_index = index - 1
        if previous_index == 0:
            new_previous_output_dir = previous_output_dir
        else:
            new_previous_output_dir = f"{os.path.splitext(previous_output_dir)[0]}_{previous_index}{os.path.splitext(output_dir)[1]}"

        previous_output_dir = new_previous_output_dir

    if os.path.exists(previous_output_dir):
        previous_process_data = read_json(previous_output_dir)
        data = previous_process_data + data[len(previous_process_data):]


    for item in tqdm(data, total=len(data),desc="Getting Cypher With CASE"):
        try:
            if "cypher_" in item.keys():
                continue
            question = item["paraphrased_questions"]
            phs = item["placeholders_value"]
            cypher_list = []
            # for time in tqdm(range(args.generate_num), total=args.generate_num, desc="generate {} for cypher".format(args.generate_num)):
            for time in range(args.generate_num):
                if args.wo_schema:
                    cypher = chain.invoke(
                        {
                            "question": question,
                            "entities": phs
                        }
                    )
                elif args.wo_ner:
                    cypher = chain.invoke(
                        {
                            "question": question,
                            "schema": graph_schema,
                        }
                    )
                else:
                    cypher = chain.invoke(
                        {
                            "question": question,
                            "schema": graph_schema,
                            "entities": phs
                         }
                    )
                cypher_list.append(cypher)

            item["cypher_"] = cypher_list

        except Exception as e:
            qid = item["qid"]
            # Log the current key and error message
            print("Error occurred with question, the qid is {}, \nerror information: \n{}".format(qid, e))
            continue  # Skip to the next key if an error occurs


    with open(output_dir, 'w') as file:
        json.dump(data, file)

    return data

if __name__ == '__main__':
    start_time = time.time()


    args = parse_args()

    # BIANXIE AI
    os.environ['OPENAI_API_KEY'] = "sk-w6DavkHl1dHAW3wkA3766dCe88D24336AfE64aC332707a99"
    os.environ["OPENAI_BASE_URL"] = "https://api.bianxie.ai/v1"

    # XIAO AI
    # os.environ['OPENAI_API_KEY'] = "sk-Ay7DiIQLUm9lOIDBzXaNcQt0NhNqHF90Z1slzU1PrDBp46s7"
    # # os.environ['OPENAI_API_KEY'] = "sk-Ay7DiIQLUm9lOIDBzXaNcQt0NhNqHF90Z1slzU1PrDBp46s7" # 4
    # os.environ["OPENAI_BASE_URL"] = "https://xiaoai.plus/v1"

    if args.GLM:
        # model = ChatGLM3()
        if os.path.exists(args.model_path):
            model = ChatGLM3(args.model_path)
        else:
            raise ValueError("The local model path needs to be provided.")
            # args.GPT = True
    elif args.GPT:
        # gpt-3.5-turbo
        model = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=args.temperature
        )
    else:
        raise ValueError("The model name needs to be provided.")

    case_path = args.case_path
    case = process_case(case_path)
    if args.emb_model:
        if not os.path.exists(args.model_path):
            raise ValueError("The embedding model does not exist.")
        embeddings = LocalEmbeddings(args.emb_model)
    else:
        embeddings = OpenAIEmbeddings()

    if args.graph_schema:
        graph_schema_path = args.graph_schema
        with open(graph_schema_path, 'r') as file:
            graph_schema = file.read()
    else:
        graph_schema = None

    example_selector = SemanticSimilarityExampleSelector.from_examples(
        case,
        embeddings,
        Chroma,
        k=args.case_num,
        input_keys=["question"]
    )

    # Define the example prompt
    example_prompt = PromptTemplate.from_template("Question: {question}\nCypher: {cypher}")

    # Define the FewShotPromptTemplate

    if args.wo_schema:
        question_prompt = FewShotPromptTemplate(
            # examples=examples,
            example_selector=example_selector,
            example_prompt=example_prompt,
            suffix="Question: {question}\nCypher:",
            prefix="""You are a Neo4j expert. Given an input question, create a Cypher query to run.
        Instructions:
        Use only the provided relationship types and properties in the schema.
        Do not use any other relationship types or properties that are not provided.
        The entities involved in the question are:
        {entities}
        Note: Do not include any explanations or apologies in your responses.
        Do not include any text except the generated Cypher statement.
        Below are a number of example of questions and their question and their corresponding Cypher queries""",
            input_variables=["question"],
        )
    elif args.wo_ner:
        question_prompt = FewShotPromptTemplate(
            # examples=examples,
            example_selector=example_selector,
            example_prompt=example_prompt,
            suffix="Question: {question}\nCypher:",
            prefix="""You are a Neo4j expert. Given an input question, create a Cypher query to run.
        Instructions:
        Use only the provided relationship types and properties in the schema.
        Do not use any other relationship types or properties that are not provided.
        Schema:
        {schema}
        Note: Do not include any explanations or apologies in your responses.
        Do not include any text except the generated Cypher statement.
        Below are a number of example of questions and their question and their corresponding Cypher queries""",
            input_variables=["question"],
        )
    else:
        question_prompt = FewShotPromptTemplate(
            # examples=examples,
            example_selector=example_selector,
            example_prompt=example_prompt,
            suffix="Question: {question}\nCypher:",
            prefix="""You are a Neo4j expert. Given an input question, create a Cypher query to run.
        Instructions:
        Use only the provided relationship types and properties in the schema.
        Do not use any other relationship types or properties that are not provided.
        Schema:
        {schema}
        The entities involved in the question are:
        {entities}
        Note: Do not include any explanations or apologies in your responses.
        Do not include any text except the generated Cypher statement.
        Below are a number of example of questions and their question and their corresponding Cypher queries""",
            input_variables=["question"],
        )

    if args.case_num == 0:
        question_prompt = PromptTemplate.from_template(
            """You are a Neo4j expert. Given an input question, create a Cypher query to run.
Instructions:
Use only the provided relationship types and properties in the schema.
Do not use any other relationship types or properties that are not provided.
The following is two example:
question 1: 在除油2加工过程中，工件GP/散热片S-G153的产能是多少？
cypher 1: MATCH p=(n)-[r:processed]->(m) WHERE n.name="GP/散热片S-G153" AND m.name="除油2" \nwith p,n,m,r.sequence as sequence\nMATCH q=(n)-[r:wp_time]->(j) where r.process_sequence=sequence\nreturn j.ProductionCapacity
question 2: 使用抗氧化剂A的工序有哪些？
cypher 2: MATCH (m)-[r:pcprocessed_using]->(n) WHERE n.name = "抗氧化剂A" RETURN m.name
Schema:
{schema}
The entities involved in the question are:
{entities}
Note: Do not include any explanations or apologies in your responses.
Do not include any text except the generated Cypher statement.
"""
        )

    chain = question_prompt | model | StrOutputParser()

    output = args.output
    if not os.path.exists(output):
        os.makedirs(output)

    data_path = args.data
    get_cypher(data_path, output, chain, args)


    # 记录结束时间
    end_time = time.time()
    # 计算运行时间
    elapsed_time = end_time - start_time
    print(f"代码运行时间: {elapsed_time} 秒")
